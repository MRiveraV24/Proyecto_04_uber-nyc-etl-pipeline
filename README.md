# Pipeline de Datos ETL para An谩lisis de Viajes de Uber en NYC (2014)

---

# Sales ETL Pipeline

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

---
<artifact identifier="readme-uber-etl" type="text/markdown" title="README.md para Repositorio GitHub">
#  Uber NYC ETL Pipeline - Arquitectura Medallion

Mostrar imagen
Mostrar imagen
Mostrar imagen
Mostrar imagen
Mostrar imagen


#### **Resumen Ejecutivo**
Describe brevemente el proyecto y sus objetivos principales.
Este proyecto establece una robusta **canalizaci贸n de datos ETL (Extracci贸n, Transformaci贸n, Carga)** para analizar los datos de viajes de Uber en la ciudad de Nueva York durante el per铆odo de **abril a septiembre de 2014**. Utilizando una **arquitectura Medall贸n (Bronze, Silver, Gold)** implementada en **Databricks Free Edition**, la soluci贸n ingesta datos crudos, los limpia y enriquece, y finalmente los agrega en un formato optimizado para el an谩lisis de negocio. El objetivo principal es proporcionar **insights accionables** sobre patrones de demanda, rendimiento de bases y distribuci贸n geogr谩fica de los viajes, permitiendo la toma de decisiones informadas.

#### **Contexto del Negocio y Objetivos**
Explica el problema de negocio y las preguntas clave que el proyecto busca responder.
La gesti贸n eficiente de una plataforma de transporte como Uber requiere una comprensi贸n profunda de los patrones de demanda y oferta. Sin un an谩lisis de datos estructurado, Uber NYC podr铆a enfrentar ineficiencia operacional, una experiencia de usuario sub贸ptima y una planificaci贸n estrat茅gica deficiente.

Este proyecto busca responder preguntas clave como:
*   驴Cu谩les son las **horas pico de demanda** de Uber en NYC y c贸mo var铆an por d铆a de la semana?
*   驴C贸mo se distribuye la demanda de viajes **geogr谩ficamente** en NYC? 驴Existen "hotspots" de recogida?
*   驴Cu谩l es el **rendimiento de cada base de Uber** en t茅rminos de volumen de viajes y actividad?
*   驴C贸mo ha evolucionado la demanda de viajes **mes a mes** durante el per铆odo analizado (abril-septiembre de 2014)?
*   驴Existen patrones de demanda distintivos entre **d铆as de semana y fines de semana**?

Los objetivos principales del proyecto son:
*   Construir una **Canalizaci贸n de Datos Confiable**.
*   Proporcionar **Datos de Alta Calidad**.
*   Generar **Insights de Negocio**.
*   Demostrar **Habilidades T茅cnicas** en ingenier铆a de datos, especialmente con Databricks y Delta Lake.

#### **Descripci贸n del Conjunto de Datos**
Detalla la fuente de datos utilizada.
El proyecto se basa en el conjunto de datos **'Uber Pickups in New York City' de Kaggle**.
*   **Periodo de los Datos**: Abril a septiembre de 2014.
*   **Volumen de Datos**: M谩s de **4.5 millones de registros** de recogidas de Uber.
*   **Formato de los Archivos**: Cada mes est谩 contenido en un archivo CSV separado.
*   **Ubicaci贸n**: Ciudad de Nueva York.

Las columnas clave en los archivos mensuales son:
*   `Date/Time`: Fecha y hora exactas de la recogida del viaje.
*   `Lat`: Latitud geogr谩fica del punto de recogida.
*   `Lon`: Longitud geogr谩fica del punto de recogida.
*   `Base`: C贸digo de la base de la empresa afiliada a la recogida de Uber.

Los archivos de origen son: `uber-raw-data-apr14.csv`, `uber-raw-data-aug14.csv`, `uber-raw-data-jul14.csv`, `uber-raw-data-jun14.csv`, `uber-raw-data-may14.csv`, `uber-raw-data-sep14.csv`.

#### **Arquitectura de la Soluci贸n: Enfoque Medall贸n**
Explica la arquitectura Medall贸n implementada.
La soluci贸n implementa una **arquitectura de lago de datos con el enfoque Medall贸n**, organizando los datos en tres capas distintas: Bronze (Crudo), Silver (Curado) y Gold (Consumible). Esta estructura promueve la calidad, la gobernanza y la facilidad de consumo de los datos.

**(Opcional: Si tienes un diagrama de arquitectura, ins茅rtalo aqu铆)**
`![Diagrama de Arquitectura Medall贸n](assets/architecture_diagram.png)`

##### **Descripci贸n de las Capas:**
*   **Capa Bronze (Raw Data)**:
    *   **Prop贸sito**: Ingesta de datos crudos desde las fuentes, preservando su formato original, actuando como un archivo hist贸rico inmutable.
    *   **Contenido**: Datos de los archivos CSV de Uber tal cual, con metadatos de auditor铆a (timestamp de ingesta, nombre del archivo fuente, versi贸n de la capa).
    *   **Formato**: Delta Lake.
*   **Capa Silver (Cleaned & Enriched)**:
    *   **Prop贸sito**: Limpieza, validaci贸n y enriquecimiento de los datos de la capa Bronze, listos para un an谩lisis m谩s profundo y la creaci贸n de caracter铆sticas.
    *   **Contenido**: Datos limpios, con tipos de datos corregidos, valores nulos manejados, duplicados eliminados y nuevas caracter铆sticas generadas (ej. hora de recogida, d铆a de la semana, tipo de d铆a, categor铆a de tiempo).
    *   **Formato**: Delta Lake.
*   **Capa Gold (Business-Ready)**:
    *   **Prop贸sito**: Agregaci贸n y modelado de datos para casos de uso espec铆ficos de negocio, optimizadas para el rendimiento de consultas y el consumo por herramientas de BI.
    *   **Contenido**: KPIs ejecutivos, m茅tricas temporales (diarias, horarias, mensuales), rendimiento por base, y datos geoespaciales.
    *   **Formato**: Delta Lake.

#### **Fases del Pipeline ETL**
Detalla cada paso del pipeline, mencionando los notebooks utilizados.
El pipeline ETL se implementa a trav茅s de una serie de notebooks de Databricks, cada uno encargado de una fase espec铆fica.

*   **1. Extracci贸n (E): Capa Bronze (`01_Uber_Bronze_Layer.ipynb`)**
    *   **Objetivo**: Ingestar los datos crudos de los archivos CSV en una tabla Delta en la capa Bronze.
    *   **Proceso**: Configuraci贸n de rutas, validaci贸n de archivos fuente, definici贸n de esquema expl铆cito, lectura de datos, adici贸n de metadatos de auditor铆a (ej. `ingestion_timestamp`, `source_file`), y carga a Delta Lake.
    *   **Validaci贸n**: Verificaci贸n del recuento de registros y muestra de datos.
*   **2. Transformaci贸n (T): Capa Silver (`02_Uber_Silver_Layer.ipynb`)**
    *   **Objetivo**: Limpiar, validar y enriquecer los datos de la capa Bronze, prepar谩ndolos para el an谩lisis.
    *   **Proceso**: Lectura de datos Bronze, an谩lisis inicial de calidad de datos, **limpieza de datos** (renombrado de columnas, conversi贸n de tipos, validaci贸n de rangos geogr谩ficos, manejo de nulos y duplicados), **ingenier铆a de caracter铆sticas** (ej. `pickup_hour`, `day_type`, `trip_id`), y adici贸n de metadatos de auditor铆a Silver.
    *   **Validaci贸n**: Verificaci贸n del esquema final, recuento de registros y muestra de datos limpios.
*   **3. Carga (L): Capa Gold (`03_Uber_Gold_Layer.ipynb`)**
    *   **Objetivo**: Crear agregaciones de negocio, KPIs y m茅tricas anal铆ticas desde la capa Silver, optimizadas para el consumo.
    *   **Proceso**: Lectura de datos Silver y creaci贸n de cinco tablas Gold especializadas:
        *   `daily_metrics` (Agregaciones Diarias)
        *   `base_hourly_metrics` (M茅tricas Horarias por Base)
        *   `monthly_kpis` (KPIs Mensuales)
        *   `spatial_metrics` (M茅tricas Geoespaciales)
        *   `executive_dashboard` (Resumen Ejecutivo Global)
    *   Cada DataFrame agregado se escribe como una tabla Delta separada en la capa Gold.
    *   **Validaci贸n**: Verificaci贸n del recuento de registros y muestra de datos de cada tabla Gold.

#### **Modelo de Datos**
Presenta el esquema de las tablas en cada capa. Puedes resumirlo o enlazar a secciones m谩s detalladas.
El modelo de datos sigue la arquitectura Medall贸n, con esquemas que evolucionan en cada capa para mejorar la calidad y la utilidad de los datos.

*   **Capa Bronze (`uber_bronze`)**: `Date/Time` (StringType), `Lat` (DoubleType), `Lon` (DoubleType), `Base` (StringType), `ingestion_timestamp` (TimestampType), `source_file` (StringType), `bronze_layer_version` (StringType).
*   **Capa Silver (`uber_silver`)**: Incluye las columnas de Bronze transformadas (`pickup_datetime`, `latitude`, `longitude`, `base_code`), m谩s numerosas **columnas enriquecidas** como `pickup_hour`, `pickup_day_of_week`, `time_category`, `day_type`, `trip_id`, y metadatos de auditor铆a Silver.
*   **Capa Gold (Tablas Agregadas)**:
    *   `daily_metrics`: Incluye m茅tricas como `total_trips`, `active_bases`, `morning_rush_trips`, y dimensiones temporales.
    *   `base_hourly_metrics`: M茅tricas por `base_code` y `pickup_hour`, como `hourly_trips`, `weekday_percentage`, `weekend_percentage`.
    *   `monthly_kpis`: KPIs mensuales como `total_monthly_trips`, `mom_growth`, `rush_hours_percentage`.
    *   `spatial_metrics`: Agregaciones por `latitude_bin` y `longitude_bin` con `total_trips`, `trip_density`.
    *   `executive_dashboard`: Resumen de alto nivel como `total_trips_period`, `avg_daily_trips`, `rush_hours_share`, `weekend_vs_weekday_ratio`.

#### **Tecnolog铆as y Herramientas**
Enumera y justifica las tecnolog铆as clave utilizadas.
*   **Databricks Free Edition**: Plataforma unificada con Spark optimizado, soporte nativo para Delta Lake, y facilidades de colaboraci贸n, ideal para procesar grandes vol煤menes de datos.
*   **Delta Lake**: Garantiza fiabilidad (transacciones ACID), esquema evolucionable, manejo de datos a escala, control de versiones y es nativo con Spark.
*   **Power BI**: Permite crear dashboards interactivos, se conecta directamente a Delta Lake y facilita la distribuci贸n de informes.

#### **Visualizaci贸n y Hallazgos Clave**
Describe las visualizaciones importantes y los insights obtenidos.
La capa Gold est谩 dise帽ada para ser directamente consumible por Power BI, permitiendo la creaci贸n de dashboards para la exploraci贸n de datos.

**Hallazgos Clave:**
*   **Crecimiento Mensual de Viajes**: Se observ贸 un crecimiento sostenido, culminando en un **aumento del 24% en septiembre** de 2014, indicando una r谩pida adopci贸n de Uber en NYC.
*   **Patrones de Demanda por Hora del D铆a**: Clara identificaci贸n de **"horas pico" entre las 17:00 y las 21:00**, crucial para la gesti贸n de la flota y tarifas din谩micas.
*   **Rendimiento por Base de Operaciones**: Las bases **B02617, B02598 y B02682 dominan el volumen de viajes**, siendo pilares de la operaci贸n.
*   **Hotspots Geogr谩ficos de Demanda**: Identificaci贸n de zonas de alta concentraci贸n de recogidas como **Midtown Manhattan, el Distrito Financiero y el East Village**, sugiriendo d贸nde concentrar la oferta de veh铆culos.

**(Opcional: Si tienes capturas de pantalla de los dashboards o visualizaciones, ins茅rtalas aqu铆)**
`![Crecimiento Mensual de Viajes](assets/monthly_growth.png)`
`![Patrones de Demanda Horaria](assets/hourly_patterns.png)`

#### **Conclusiones**
Resume los logros del proyecto.
Este proyecto ha demostrado la capacidad de construir una canalizaci贸n de datos ETL robusta y escalable, permitiendo la ingesta eficiente, la transformaci贸n de calidad, el modelado para el negocio y la habilitaci贸n de insights cr铆ticos sobre la demanda de Uber en NYC.

#### **Futuras Mejoras y Pasos Siguientes**
Sugiere c贸mo el proyecto podr铆a evolucionar.
Para seguir evolucionando y extraer a煤n m谩s valor de los datos, se proponen:
*   **Integraci贸n de Datos Adicionales**: Incorporar datos meteorol贸gicos, eventos especiales, datos de tr谩fico o an谩lisis de sentimiento de redes sociales.
*   **An谩lisis Predictivo**: Desarrollar modelos de Machine Learning para pronosticar la demanda o detectar anomal铆as.
*   **Optimizaci贸n de Costos y Rendimiento**: Implementar particionamiento inteligente, Z-Ordering, o migrar a streaming de datos para an谩lisis casi instant谩neo.
*   **Gobernanza y Monitoreo**: Establecer controles automatizados de calidad de datos y un cat谩logo de datos.
*   **Expansi贸n a la Plataforma Azure**: Utilizar Azure Data Factory para orquestaci贸n, Azure Databricks como motor de procesamiento, Azure SQL Database para almacenamiento final, Power BI para visualizaci贸n, Azure Key Vault para gesti贸n segura de credenciales, y Azure DevOps para CI/CD.

#### **C贸mo Ejecutar el Proyecto (Configuraci贸n)**
Esta secci贸n es crucial para un portafolio de GitHub, ya que permite a otros replicar tu trabajo.

1.  **Requisitos**:
    *   Una cuenta de **Databricks Free Edition** (o superior).
    *   Acceso a los archivos CSV de 'Uber Pickups in New York City' de Kaggle (disponibles en [https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city](https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city)).
    *   (Opcional) Power BI Desktop para la visualizaci贸n final.

2.  **Configuraci贸n del Entorno en Databricks**:
    *   **Crear un cl煤ster**: Configura un cl煤ster de Spark compatible con Databricks Runtime.
    *   **Cargar los datos crudos**: Sube los seis archivos CSV (`uber-raw-data-apr14.csv` a `uber-raw-data-sep14.csv`) a una ubicaci贸n accesible dentro de Databricks (ej. DBFS o un montaje de almacenamiento en la nube). **Aseg煤rate de ajustar las rutas de origen en el notebook `01_Uber_Bronze_Layer.ipynb` para que apunten a la ubicaci贸n correcta de tus archivos CSV.**
    *   **Configurar Vol煤menes**: Si est谩s utilizando la estructura de vol煤menes de Databricks (como `/Volumes/workspace/default/uber_etl_azure/`), aseg煤rate de que estos est茅n configurados o adapta las rutas en los notebooks.

3.  **Ejecuci贸n de los Notebooks**:
    *   **Paso 1: Capa Bronze**
        *   Abre `notebooks/01_Uber_Bronze_Layer.ipynb`.
        *   Ejecuta todas las celdas para ingestar los datos crudos y crear la tabla `uber_bronze`.
    *   **Paso 2: Capa Silver**
        *   Abre `notebooks/02_Uber_Silver_Layer.ipynb`.
        *   Ejecuta todas las celdas para limpiar, validar y enriquecer los datos, creando la tabla `uber_silver`.
    *   **Paso 3: Capa Gold**
        *   Abre `notebooks/03_Uber_Gold_Layer.ipynb`.
        *   Ejecuta todas las celdas para crear las cinco tablas agregadas de la capa Gold (`daily_metrics`, `base_hourly_metrics`, `monthly_kpis`, `spatial_metrics`, `executive_dashboard`).
    *   **Paso 4: Visualizaci贸n (Databricks)**
        *   Abre `notebooks/04_Uber_Dashboard_Databricks.ipynb`.
        *   Ejecuta las celdas para explorar los datos de la capa Gold y ver visualizaciones preliminares.

4.  **Conexi贸n con Power BI (Opcional)**:
    *   Conecta Power BI a las tablas Delta de la capa Gold a trav茅s del `Databricks SQL Analytics Endpoint`.


---

