# ğŸš– Uber NYC ETL Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.x-orange.svg)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-2.x-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Pipeline ETL profesional con Arquitectura Medallion para anÃ¡lisis de 4.5M+ viajes de Uber en NYC**

[CaracterÃ­sticas](#-caracterÃ­sticas-principales) â€¢ [InstalaciÃ³n](#-instalaciÃ³n) â€¢ [Uso](#-uso-del-pipeline) â€¢ [Arquitectura](#-arquitectura-medallion) â€¢ [Resultados](#-resultados)

</div>

---

## ğŸ“‹ DescripciÃ³n

Pipeline ETL End To End para el anÃ¡lisis de **4.5+ millones de viajes de Uber en Nueva York** (abril-septiembre 2014), implementando una arquitectura de lago de datos tipo **Medallion** con tres capas (Bronze, Silver, Gold). 

Este proyecto demuestra capacidades avanzadas de ingenierÃ­a de datos utilizando tecnologÃ­as cloud-native y mejores prÃ¡cticas de la industria para transformar datos crudos en insights accionables de negocio.

### ğŸ¯ Objetivos

**Objetivos de Negocio:**
- Analizar patrones de demanda temporal y geogrÃ¡fica de Uber NYC
- Identificar hotspots de alta concentraciÃ³n de viajes
- Optimizar operaciones mediante insights de horas pico y rendimiento por base
- Generar KPIs ejecutivos para toma de decisiones estratÃ©gicas

**Objetivos TÃ©cnicos:**
- Construir un pipeline ETL escalable y mantenible
- Implementar mejores prÃ¡cticas de Data Engineering
- Demostrar experiencia con Databricks, PySpark y Delta Lake
- Crear un portafolio profesional de ingenierÃ­a de datos

---

## âœ¨ CaracterÃ­sticas Principales

```
âœ… Arquitectura Medallion completa (Bronze â†’ Silver â†’ Gold)
âœ… Procesamiento distribuido con Apache Spark
âœ… Transacciones ACID mediante Delta Lake
âœ… IngenierÃ­a de 12 caracterÃ­sticas automatizadas
âœ… Validaciones de Data Quality en cada capa
âœ… 5 tablas Gold especializadas para analytics
âœ… Dashboard ejecutivo con SQL analytics
âœ… 100% reproducible en Databricks Free Edition
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### Lenguajes y Frameworks

| CategorÃ­a | TecnologÃ­as |
|-----------|------------|
| **Procesamiento** | ![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white) ![Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=flat&logo=apache-spark&logoColor=white) |
| **Storage** | ![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=flat) |
| **AnÃ¡lisis** | ![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas&logoColor=white) ![Plotly](https://img.shields.io/badge/Plotly-3F4F75?style=flat&logo=plotly&logoColor=white) |
| **Testing** | ![Pytest](https://img.shields.io/badge/Pytest-0A9EDC?style=flat&logo=pytest&logoColor=white) |

### Componentes Clave

```python
# Procesamiento de Datos
â”œâ”€â”€ PySpark 3.x          # Motor de procesamiento distribuido
â”œâ”€â”€ Delta Lake 2.x       # Transacciones ACID y versionado
â”œâ”€â”€ Pandas 2.x           # AnÃ¡lisis de datos locales
â”‚
# VisualizaciÃ³n
â”œâ”€â”€ Plotly 5.x           # Visualizaciones interactivas
â”œâ”€â”€ Matplotlib 3.x       # GrÃ¡ficos estÃ¡ticos
â”œâ”€â”€ Seaborn 0.12+        # Visualizaciones estadÃ­sticas
â”‚
# Testing y Calidad
â”œâ”€â”€ pytest 7.x           # Framework de testing
â””â”€â”€ Great Expectations   # Data quality validations
```

### Plataforma

- **Databricks Free Edition** - Plataforma de procesamiento
- **Delta Lake** - Storage layer con ACID transactions
- **Apache Spark** - Motor de procesamiento distribuido
- **Jupyter Notebooks** - Desarrollo interactivo

---

## ğŸ“ Estructura del Proyecto

```
uber-nyc-etl-pipeline/
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                          # Notebooks Databricks (nÃºcleo del pipeline)
â”‚   â”œâ”€â”€ 01_Bronze_Layer.ipynb             # Ingesta de 4.5M registros
â”‚   â”œâ”€â”€ 02_Silver_Layer.ipynb             # Limpieza + 12 features
â”‚   â”œâ”€â”€ 03_Gold_Layer.ipynb               # 5 tablas agregadas
â”‚   â””â”€â”€ 04_Dashboard_Analytics.ipynb      # SQL Analytics + Insights
â”‚
â”œâ”€â”€ ğŸ src/                                # CÃ³digo Python modular
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ bronze_layer.py               # Funciones de ingesta
â”‚   â”‚   â”œâ”€â”€ silver_layer.py               # Transformaciones
â”‚   â”‚   â””â”€â”€ gold_layer.py                 # Agregaciones
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_quality.py               # Validaciones
â”‚   â”‚   â”œâ”€â”€ transformations.py            # Transformaciones reutilizables
â”‚   â”‚   â””â”€â”€ helpers.py                    # Funciones auxiliares
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.py                    # Configuraciones
â”‚
â”œâ”€â”€ ğŸ§ª tests/                              # Suite de testing
â”‚   â”œâ”€â”€ test_bronze_layer.py
â”‚   â”œâ”€â”€ test_silver_layer.py
â”‚   â”œâ”€â”€ test_gold_layer.py
â”‚   â””â”€â”€ test_data_quality.py
â”‚
â”œâ”€â”€ ğŸ“Š data/
â”‚   â”œâ”€â”€ raw/                               # CSVs originales (6 archivos)
â”‚   â”œâ”€â”€ sample/                            # Datos de muestra
â”‚   â””â”€â”€ processed/                         # Outputs locales
â”‚
â”œâ”€â”€ ğŸ“š docs/                               # DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â”œâ”€â”€ pipeline_workflow.md
â”‚   â””â”€â”€ images/
â”‚
â”œâ”€â”€ ğŸ“„ reports/
â”‚   â””â”€â”€ Report_Pipeline_Uber_2014.pdf     # Informe completo
â”‚
â””â”€â”€ ğŸ”§ scripts/
    â”œâ”€â”€ setup_environment.sh
    â”œâ”€â”€ run_pipeline.py
    â””â”€â”€ validate_data.py
```

---

## ğŸ—ï¸ Arquitectura Medallion

```mermaid
graph LR
    A[ğŸ“ CSV Files<br/>6 archivos<br/>4.5M registros] -->|Ingesta| B[ğŸ¥‰ BRONZE<br/>uber_bronze<br/>Raw Data]
    B -->|Limpieza +<br/>ValidaciÃ³n| C[ğŸ¥ˆ SILVER<br/>uber_silver<br/>12 features]
    C -->|AgregaciÃ³n<br/>Especializada| D[ğŸ¥‡ GOLD<br/>5 Tablas]
    D --> E[ğŸ“Š Analytics<br/>Dashboard]
    
    style A fill:#e1f5ff
    style B fill:#cd7f32
    style C fill:#c0c0c0
    style D fill:#ffd700
    style E fill:#90EE90
```

### ğŸ¥‰ Bronze Layer - Raw Data Ingestion

| Aspecto | Detalle |
|---------|---------|
| **Input** | 6 archivos CSV (Abril-Septiembre 2014) |
| **Proceso** | Ingesta sin modificaciones + metadatos de auditorÃ­a |
| **Output** | Tabla Delta `uber_bronze` (4,534,327 registros) |
| **Formato** | Delta Lake (ACID transactions) |

### ğŸ¥ˆ Silver Layer - Cleaned & Enriched

| Aspecto | Detalle |
|---------|---------|
| **Input** | Tabla `uber_bronze` |
| **Transformaciones** | ConversiÃ³n de tipos, validaciÃ³n geogrÃ¡fica, eliminaciÃ³n de duplicados |
| **Feature Engineering** | CreaciÃ³n de 12 caracterÃ­sticas temporales |
| **Output** | Tabla Delta `uber_silver` (4,502,417 registros) |
| **Tasa de RetenciÃ³n** | 99.3% (31,910 registros filtrados) |

**Features Creadas:**
- CaracterÃ­sticas temporales: `year`, `month`, `day`, `hour`, `day_of_week`, `is_weekend`
- CategorizaciÃ³n temporal: `time_of_day`, `hour_category`, `day_part`
- MÃ©tricas geogrÃ¡ficas: `lat_rounded`, `lon_rounded`
- Identificadores: `trip_id`

### ğŸ¥‡ Gold Layer - Business-Ready Analytics

| Tabla | Registros | DescripciÃ³n |
|-------|-----------|-------------|
| `daily_metrics` | 183 | Agregaciones diarias |
| `base_hourly_metrics` | 120 | Performance por base/hora |
| `monthly_kpis` | 6 | Tendencias mensuales con MoM growth |
| `spatial_metrics` | 1,684 | AnÃ¡lisis geoespacial |
| `executive_dashboard` | 1 | KPIs globales |

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.9 o superior
- Cuenta en [Databricks Free Edition](https://www.databricks.com/learn/free-edition) (gratuita)
- Git instalado

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/MRiveraV24/Proyecto_04_uber-nyc-etl-pipeline.git
cd Proyecto_04_uber-nyc-etl-pipeline
```

### Paso 2: Configurar Entorno Virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Windows:
venv\Scripts\activate
# En macOS/Linux:
source venv/bin/activate
```

### Paso 3: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Paso 4: Configurar Databricks

1. Crear cuenta gratuita en [Databricks Free Edition](https://www.databricks.com/learn/free-edition/)
2. Crear un nuevo Workspace
3. Importar los notebooks desde la carpeta `notebooks/`
4. Subir los datos CSV a DBFS (Databricks File System)

### Paso 5: Configurar Rutas

Editar `src/config/settings.py` con tus rutas de Databricks:

```python
# Rutas de datos en DBFS
SOURCE_PATH = "/Volumes/workspace/default/uber_etl_azure/"
BRONZE_PATH = "/Volumes/workspace/default/uber_etl_azure/bronze/"
SILVER_PATH = "/Volumes/workspace/default/uber_etl_azure/silver/"
GOLD_PATH = "/Volumes/workspace/default/uber_etl_azure/gold/"
```

---

## ğŸ’» Uso del Pipeline

### OpciÃ³n 1: EjecuciÃ³n en Databricks (Recomendado)

#### ğŸ¥‰ Paso 1: Bronze Layer - Ingesta

```python
# Ejecutar: 01_Bronze_Layer.ipynb
# âœ… Ingesta 4.5M+ registros desde 6 archivos CSV
# âœ… Preserva estructura original sin modificaciones
# ğŸ“Š Output: Tabla Delta uber_bronze (formato ACID)
```

#### ğŸ¥ˆ Paso 2: Silver Layer - TransformaciÃ³n

```python
# Ejecutar: 02_Silver_Layer.ipynb
# âœ… ConversiÃ³n de tipos de datos (timestamp, double)
# âœ… ValidaciÃ³n geogrÃ¡fica (NYC bounds)
# âœ… EliminaciÃ³n de duplicados (31,910 registros)
# âœ… CreaciÃ³n de 12 features temporales y geogrÃ¡ficas
# ğŸ“Š Output: Tabla Delta uber_silver (4.5M registros limpios)
```

#### ğŸ¥‡ Paso 3: Gold Layer - AgregaciÃ³n

```python
# Ejecutar: 03_Gold_Layer.ipynb
# âœ… CreaciÃ³n de 5 tablas especializadas:
#    â€¢ daily_metrics: Agregaciones diarias
#    â€¢ base_hourly_metrics: Performance por base/hora
#    â€¢ monthly_kpis: KPIs mensuales con MoM growth
#    â€¢ spatial_metrics: AnÃ¡lisis geoespacial (1,684 zonas)
#    â€¢ executive_dashboard: Resumen ejecutivo global
```

#### ğŸ“Š Paso 4: Dashboard y Analytics

```python
# Ejecutar: 04_Dashboard_Analytics.ipynb
# âœ… AnÃ¡lisis SQL interactivo
# âœ… Visualizaciones con Plotly
# âœ… ExtracciÃ³n de insights de negocio
```

### OpciÃ³n 2: EjecuciÃ³n Local (Testing)

```bash
# Ejecutar pipeline completo
python scripts/run_pipeline.py

# Validar calidad de datos
python scripts/validate_data.py

# Ejecutar tests
pytest tests/ -v
```

---

## ğŸ§ª Testing

### Ejecutar Suite Completa

```bash
pytest tests/ -v --cov=src
```

### Tests por Capa

```bash
# Test Bronze Layer
pytest tests/test_bronze_layer.py -v

# Test Silver Layer
pytest tests/test_silver_layer.py -v

# Test Gold Layer
pytest tests/test_gold_layer.py -v

# Test Data Quality
pytest tests/test_data_quality.py -v
```

### Cobertura de Testing

```
âœ… ValidaciÃ³n de ingesta de datos
âœ… Transformaciones de limpieza
âœ… IngenierÃ­a de caracterÃ­sticas
âœ… Agregaciones de negocio
âœ… Calidad de datos end-to-end
```

---

## ğŸ“Š Resultados

### MÃ©tricas del Pipeline

| MÃ©trica | Valor | DescripciÃ³n |
|---------|-------|-------------|
| **Registros Procesados** | 4,502,417 | Viajes de Uber NYC (Abr-Sep 2014) |
| **Datos Brutos (Bronze)** | 4,534,327 | 100% de datos originales preservados |
| **Datos Curados (Silver)** | 4,502,417 | 99.3% de retenciÃ³n post-limpieza |
| **Tablas Gold** | 5 | Especializadas por caso de uso |
| **Features Creadas** | 12 | Temporales, geogrÃ¡ficas y categÃ³ricas |
| **Zonas GeogrÃ¡ficas** | 1,684 | AnÃ¡lisis espacial granular |
| **Crecimiento PerÃ­odo** | +81.5% | Abril â†’ Septiembre 2014 |

### KPIs de Negocio

#### ğŸ“ˆ Rendimiento Operacional

```
Promedio de viajes diarios:   24,603 viajes/dÃ­a
Bases activas:                 5 bases operacionales
Periodo de operaciÃ³n:          183 dÃ­as continuos
Cobertura horaria:             24/7
```

#### â° AnÃ¡lisis Temporal

```
ConcentraciÃ³n rush hours:      42.5% de viajes
Actividad nocturna:            24.2% de viajes
Ratio Weekend/Weekday:         0.33
Mes mÃ¡s activo:                Septiembre (1,020,300 viajes)
```

#### ğŸ—ºï¸ DistribuciÃ³n GeogrÃ¡fica

```
Centro geogrÃ¡fico:             (40.7387Â°, -73.9742Â°)
Cobertura latitudinal:         0.3918Â°
Cobertura longitudinal:        0.5587Â°
Top Hotspot:                   Zona 40.75, -73.98 (222,857 viajes)
```

---

## ğŸ”® PrÃ³ximas Mejoras

### ğŸ”§ Mejoras TÃ©cnicas

- [ ] **Streaming de datos** con Structured Streaming de Spark
- [ ] **Particionamiento optimizado** por fecha y base_code
- [ ] **Z-Ordering** en Delta Lake para queries mÃ¡s rÃ¡pidas
- [ ] **Data Quality Automation** con Great Expectations
- [ ] **CI/CD Pipeline** con GitHub Actions
- [ ] **Monitoring & Alerting** con Databricks SQL Analytics

### ğŸ“Š Mejoras de AnÃ¡lisis

- [ ] **IntegraciÃ³n de datos climÃ¡ticos** (correlaciÃ³n lluvia/demanda)
- [ ] **AnÃ¡lisis de eventos** (conciertos, deportes)
- [ ] **Modelos predictivos** (forecasting de demanda)
- [ ] **DetecciÃ³n de anomalÃ­as** con ML
- [ ] **AnÃ¡lisis de sentimiento** de redes sociales

### â˜ï¸ MigraciÃ³n a Azure

- [ ] **Azure Data Factory** para orquestaciÃ³n
- [ ] **Azure Databricks** (producciÃ³n)
- [ ] **Azure SQL Database** para capa Gold
- [ ] **Azure Key Vault** para secrets management
- [ ] **Azure DevOps** para CI/CD

---

## ğŸ“š DocumentaciÃ³n Adicional

- [Arquitectura Detallada](docs/architecture.md)
- [Diccionario de Datos](docs/data_dictionary.md)
- [Workflow del Pipeline](docs/pipeline_workflow.md)
- [Reporte TÃ©cnico Completo](reports/Report_Pipeline_Uber_2014.pdf)

---

## ğŸ“œ Licencia

Este proyecto estÃ¡ licenciado bajo la **MIT License** - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ‘¤ Autor

**Marcelo Rivera Vega**

[![Portfolio](https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white)](https://tu-portfolio.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/marcelo-rivera-vega/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/MRiveraV24)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:tu.marcelo.rivera.vega@gmail.com )

---

## ğŸ™ Agradecimientos

- **Databricks Free Edition** por proporcionar infraestructura gratuita
- **Kaggle** por el dataset de Uber NYC
- **Delta Lake** por su excelente documentaciÃ³n
- **Apache Spark Community** por el framework robusto
- Comunidad de **Data Engineering** en LinkedIn y Medium por inspiraciÃ³n y mejores prÃ¡cticas

---

## ğŸ“– Referencias

### DocumentaciÃ³n TÃ©cnica
- [Delta Lake Documentation](https://docs.delta.io/)
- [Databricks Best Practices](https://docs.databricks.com/best-practices/index.html)
- [Apache Spark Guide](https://spark.apache.org/docs/latest/)
- [Medallion Architecture Pattern](https://www.databricks.com/glossary/medallion-architecture)

### Dataset
- [Uber Pickups in NYC - Kaggle](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city)

---

<div align="center">

### â­ Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella en GitHub â­

[![GitHub stars](https://img.shields.io/github/stars/tu-usuario/uber-nyc-etl-pipeline?style=social)](https://github.com/tu-usuario/uber-nyc-etl-pipeline/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/tu-usuario/uber-nyc-etl-pipeline?style=social)](https://github.com/tu-usuario/uber-nyc-etl-pipeline/network/members)

**[ğŸ“ Reportar Bug](https://github.com/tu-usuario/uber-nyc-etl-pipeline/issues)** â€¢ **[ğŸ’¡ Solicitar Feature](https://github.com/tu-usuario/uber-nyc-etl-pipeline/issues)** â€¢ **[ğŸ¤ Contribuir](CONTRIBUTING.md)**

---

*Ãšltima actualizaciÃ³n: Octubre 2024*

</div>
