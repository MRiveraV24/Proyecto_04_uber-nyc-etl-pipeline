# Pipeline de Datos ETL para An√°lisis de Viajes de Uber en NYC (2014)

---

# Sales ETL Pipeline

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

---
<artifact identifier="readme-uber-etl" type="text/markdown" title="README.md para Repositorio GitHub">
# üöñ Uber NYC ETL Pipeline - Arquitectura Medallion

üìã Descripci√≥n del Proyecto
Pipeline ETL profesional de extremo a extremo para el an√°lisis de 4.5+ millones de viajes de Uber en NYC (2014), implementando una arquitectura de lago de datos tipo Medallion con tres capas (Bronze, Silver, Gold). El proyecto demuestra capacidades avanzadas de ingenier√≠a de datos utilizando tecnolog√≠as cloud-native y mejores pr√°cticas de la industria.
üéØ Caracter√≠sticas Principales

‚úÖ Arquitectura Medallion completa (Bronze ‚Üí Silver ‚Üí Gold)
‚úÖ Procesamiento distribuido con Apache Spark
‚úÖ Transacciones ACID mediante Delta Lake
‚úÖ Ingenier√≠a de caracter√≠sticas automatizada
‚úÖ Data Quality validations en cada capa
‚úÖ 5 tablas Gold especializadas para analytics
‚úÖ Dashboard ejecutivo con SQL analytics
‚úÖ 100% reproducible en Databricks Community Edition


üéØ Objetivos del Proyecto
Objetivos de Negocio

Analizar patrones de demanda temporal y geogr√°fica de Uber NYC
Identificar hotspots de alta concentraci√≥n de viajes
Optimizar operaciones mediante insights de horas pico y rendimiento por base
Generar KPIs ejecutivos para toma de decisiones estrat√©gicas

Objetivos T√©cnicos

Construir un pipeline ETL escalable y mantenible
Implementar mejores pr√°cticas de Data Engineering
Demostrar experiencia con Databricks, Spark y Delta Lake
Crear un portafolio profesional de ingenier√≠a de datos


#### **Resumen Ejecutivo**
Describe brevemente el proyecto y sus objetivos principales.
Este proyecto establece una robusta **canalizaci√≥n de datos ETL (Extracci√≥n, Transformaci√≥n, Carga)** para analizar los datos de viajes de Uber en la ciudad de Nueva York durante el per√≠odo de **abril a septiembre de 2014**. Utilizando una **arquitectura Medall√≥n (Bronze, Silver, Gold)** implementada en **Databricks Free Edition**, la soluci√≥n ingesta datos crudos, los limpia y enriquece, y finalmente los agrega en un formato optimizado para el an√°lisis de negocio. El objetivo principal es proporcionar **insights accionables** sobre patrones de demanda, rendimiento de bases y distribuci√≥n geogr√°fica de los viajes, permitiendo la toma de decisiones informadas.

#### **Contexto del Negocio y Objetivos**
Explica el problema de negocio y las preguntas clave que el proyecto busca responder.
La gesti√≥n eficiente de una plataforma de transporte como Uber requiere una comprensi√≥n profunda de los patrones de demanda y oferta. Sin un an√°lisis de datos estructurado, Uber NYC podr√≠a enfrentar ineficiencia operacional, una experiencia de usuario sub√≥ptima y una planificaci√≥n estrat√©gica deficiente.

Este proyecto busca responder preguntas clave como:
*   ¬øCu√°les son las **horas pico de demanda** de Uber en NYC y c√≥mo var√≠an por d√≠a de la semana?
*   ¬øC√≥mo se distribuye la demanda de viajes **geogr√°ficamente** en NYC? ¬øExisten "hotspots" de recogida?
*   ¬øCu√°l es el **rendimiento de cada base de Uber** en t√©rminos de volumen de viajes y actividad?
*   ¬øC√≥mo ha evolucionado la demanda de viajes **mes a mes** durante el per√≠odo analizado (abril-septiembre de 2014)?
*   ¬øExisten patrones de demanda distintivos entre **d√≠as de semana y fines de semana**?

Los objetivos principales del proyecto son:
*   Construir una **Canalizaci√≥n de Datos Confiable**.
*   Proporcionar **Datos de Alta Calidad**.
*   Generar **Insights de Negocio**.
*   Demostrar **Habilidades T√©cnicas** en ingenier√≠a de datos, especialmente con Databricks y Delta Lake.

#### **Descripci√≥n del Conjunto de Datos**
Detalla la fuente de datos utilizada.
El proyecto se basa en el conjunto de datos **'Uber Pickups in New York City' de Kaggle**.
*   **Periodo de los Datos**: Abril a septiembre de 2014.
*   **Volumen de Datos**: M√°s de **4.5 millones de registros** de recogidas de Uber.
*   **Formato de los Archivos**: Cada mes est√° contenido en un archivo CSV separado.
*   **Ubicaci√≥n**: Ciudad de Nueva York.

Las columnas clave en los archivos mensuales son:
*   `Date/Time`: Fecha y hora exactas de la recogida del viaje.
*   `Lat`: Latitud geogr√°fica del punto de recogida.
*   `Lon`: Longitud geogr√°fica del punto de recogida.
*   `Base`: C√≥digo de la base de la empresa afiliada a la recogida de Uber.

Los archivos de origen son: `uber-raw-data-apr14.csv`, `uber-raw-data-aug14.csv`, `uber-raw-data-jul14.csv`, `uber-raw-data-jun14.csv`, `uber-raw-data-may14.csv`, `uber-raw-data-sep14.csv`.

#### **Arquitectura de la Soluci√≥n: Enfoque Medall√≥n**
Explica la arquitectura Medall√≥n implementada.
La soluci√≥n implementa una **arquitectura de lago de datos con el enfoque Medall√≥n**, organizando los datos en tres capas distintas: Bronze (Crudo), Silver (Curado) y Gold (Consumible). Esta estructura promueve la calidad, la gobernanza y la facilidad de consumo de los datos.

**(Opcional: Si tienes un diagrama de arquitectura, ins√©rtalo aqu√≠)**
`![Diagrama de Arquitectura Medall√≥n](assets/architecture_diagram.png)`

##### **Descripci√≥n de las Capas:**
*   **Capa Bronze (Raw Data)**:
    *   **Prop√≥sito**: Ingesta de datos crudos desde las fuentes, preservando su formato original, actuando como un archivo hist√≥rico inmutable.
    *   **Contenido**: Datos de los archivos CSV de Uber tal cual, con metadatos de auditor√≠a (timestamp de ingesta, nombre del archivo fuente, versi√≥n de la capa).
    *   **Formato**: Delta Lake.
*   **Capa Silver (Cleaned & Enriched)**:
    *   **Prop√≥sito**: Limpieza, validaci√≥n y enriquecimiento de los datos de la capa Bronze, listos para un an√°lisis m√°s profundo y la creaci√≥n de caracter√≠sticas.
    *   **Contenido**: Datos limpios, con tipos de datos corregidos, valores nulos manejados, duplicados eliminados y nuevas caracter√≠sticas generadas (ej. hora de recogida, d√≠a de la semana, tipo de d√≠a, categor√≠a de tiempo).
    *   **Formato**: Delta Lake.
*   **Capa Gold (Business-Ready)**:
    *   **Prop√≥sito**: Agregaci√≥n y modelado de datos para casos de uso espec√≠ficos de negocio, optimizadas para el rendimiento de consultas y el consumo por herramientas de BI.
    *   **Contenido**: KPIs ejecutivos, m√©tricas temporales (diarias, horarias, mensuales), rendimiento por base, y datos geoespaciales.
    *   **Formato**: Delta Lake.

#### **Fases del Pipeline ETL**
Detalla cada paso del pipeline, mencionando los notebooks utilizados.
El pipeline ETL se implementa a trav√©s de una serie de notebooks de Databricks, cada uno encargado de una fase espec√≠fica.

*   **1. Extracci√≥n (E): Capa Bronze (`01_Uber_Bronze_Layer.ipynb`)**
    *   **Objetivo**: Ingestar los datos crudos de los archivos CSV en una tabla Delta en la capa Bronze.
    *   **Proceso**: Configuraci√≥n de rutas, validaci√≥n de archivos fuente, definici√≥n de esquema expl√≠cito, lectura de datos, adici√≥n de metadatos de auditor√≠a (ej. `ingestion_timestamp`, `source_file`), y carga a Delta Lake.
    *   **Validaci√≥n**: Verificaci√≥n del recuento de registros y muestra de datos.
*   **2. Transformaci√≥n (T): Capa Silver (`02_Uber_Silver_Layer.ipynb`)**
    *   **Objetivo**: Limpiar, validar y enriquecer los datos de la capa Bronze, prepar√°ndolos para el an√°lisis.
    *   **Proceso**: Lectura de datos Bronze, an√°lisis inicial de calidad de datos, **limpieza de datos** (renombrado de columnas, conversi√≥n de tipos, validaci√≥n de rangos geogr√°ficos, manejo de nulos y duplicados), **ingenier√≠a de caracter√≠sticas** (ej. `pickup_hour`, `day_type`, `trip_id`), y adici√≥n de metadatos de auditor√≠a Silver.
    *   **Validaci√≥n**: Verificaci√≥n del esquema final, recuento de registros y muestra de datos limpios.
*   **3. Carga (L): Capa Gold (`03_Uber_Gold_Layer.ipynb`)**
    *   **Objetivo**: Crear agregaciones de negocio, KPIs y m√©tricas anal√≠ticas desde la capa Silver, optimizadas para el consumo.
    *   **Proceso**: Lectura de datos Silver y creaci√≥n de cinco tablas Gold especializadas:
        *   `daily_metrics` (Agregaciones Diarias)
        *   `base_hourly_metrics` (M√©tricas Horarias por Base)
        *   `monthly_kpis` (KPIs Mensuales)
        *   `spatial_metrics` (M√©tricas Geoespaciales)
        *   `executive_dashboard` (Resumen Ejecutivo Global)
    *   Cada DataFrame agregado se escribe como una tabla Delta separada en la capa Gold.
    *   **Validaci√≥n**: Verificaci√≥n del recuento de registros y muestra de datos de cada tabla Gold.

#### **Modelo de Datos**
Presenta el esquema de las tablas en cada capa. Puedes resumirlo o enlazar a secciones m√°s detalladas.
El modelo de datos sigue la arquitectura Medall√≥n, con esquemas que evolucionan en cada capa para mejorar la calidad y la utilidad de los datos.

*   **Capa Bronze (`uber_bronze`)**: `Date/Time` (StringType), `Lat` (DoubleType), `Lon` (DoubleType), `Base` (StringType), `ingestion_timestamp` (TimestampType), `source_file` (StringType), `bronze_layer_version` (StringType).
*   **Capa Silver (`uber_silver`)**: Incluye las columnas de Bronze transformadas (`pickup_datetime`, `latitude`, `longitude`, `base_code`), m√°s numerosas **columnas enriquecidas** como `pickup_hour`, `pickup_day_of_week`, `time_category`, `day_type`, `trip_id`, y metadatos de auditor√≠a Silver.
*   **Capa Gold (Tablas Agregadas)**:
    *   `daily_metrics`: Incluye m√©tricas como `total_trips`, `active_bases`, `morning_rush_trips`, y dimensiones temporales.
    *   `base_hourly_metrics`: M√©tricas por `base_code` y `pickup_hour`, como `hourly_trips`, `weekday_percentage`, `weekend_percentage`.
    *   `monthly_kpis`: KPIs mensuales como `total_monthly_trips`, `mom_growth`, `rush_hours_percentage`.
    *   `spatial_metrics`: Agregaciones por `latitude_bin` y `longitude_bin` con `total_trips`, `trip_density`.
    *   `executive_dashboard`: Resumen de alto nivel como `total_trips_period`, `avg_daily_trips`, `rush_hours_share`, `weekend_vs_weekday_ratio`.

#### **Tecnolog√≠as y Herramientas**
Enumera y justifica las tecnolog√≠as clave utilizadas.
*   **Databricks Free Edition**: Plataforma unificada con Spark optimizado, soporte nativo para Delta Lake, y facilidades de colaboraci√≥n, ideal para procesar grandes vol√∫menes de datos.
*   **Delta Lake**: Garantiza fiabilidad (transacciones ACID), esquema evolucionable, manejo de datos a escala, control de versiones y es nativo con Spark.
*   **Power BI**: Permite crear dashboards interactivos, se conecta directamente a Delta Lake y facilita la distribuci√≥n de informes.

#### **Visualizaci√≥n y Hallazgos Clave**
Describe las visualizaciones importantes y los insights obtenidos.
La capa Gold est√° dise√±ada para ser directamente consumible por Power BI, permitiendo la creaci√≥n de dashboards para la exploraci√≥n de datos.

**Hallazgos Clave:**
*   **Crecimiento Mensual de Viajes**: Se observ√≥ un crecimiento sostenido, culminando en un **aumento del 24% en septiembre** de 2014, indicando una r√°pida adopci√≥n de Uber en NYC.
*   **Patrones de Demanda por Hora del D√≠a**: Clara identificaci√≥n de **"horas pico" entre las 17:00 y las 21:00**, crucial para la gesti√≥n de la flota y tarifas din√°micas.
*   **Rendimiento por Base de Operaciones**: Las bases **B02617, B02598 y B02682 dominan el volumen de viajes**, siendo pilares de la operaci√≥n.
*   **Hotspots Geogr√°ficos de Demanda**: Identificaci√≥n de zonas de alta concentraci√≥n de recogidas como **Midtown Manhattan, el Distrito Financiero y el East Village**, sugiriendo d√≥nde concentrar la oferta de veh√≠culos.

**(Opcional: Si tienes capturas de pantalla de los dashboards o visualizaciones, ins√©rtalas aqu√≠)**
`![Crecimiento Mensual de Viajes](assets/monthly_growth.png)`
`![Patrones de Demanda Horaria](assets/hourly_patterns.png)`

#### **Conclusiones**
Resume los logros del proyecto.
Este proyecto ha demostrado la capacidad de construir una canalizaci√≥n de datos ETL robusta y escalable, permitiendo la ingesta eficiente, la transformaci√≥n de calidad, el modelado para el negocio y la habilitaci√≥n de insights cr√≠ticos sobre la demanda de Uber en NYC.

#### **Futuras Mejoras y Pasos Siguientes**
Sugiere c√≥mo el proyecto podr√≠a evolucionar.
Para seguir evolucionando y extraer a√∫n m√°s valor de los datos, se proponen:
*   **Integraci√≥n de Datos Adicionales**: Incorporar datos meteorol√≥gicos, eventos especiales, datos de tr√°fico o an√°lisis de sentimiento de redes sociales.
*   **An√°lisis Predictivo**: Desarrollar modelos de Machine Learning para pronosticar la demanda o detectar anomal√≠as.
*   **Optimizaci√≥n de Costos y Rendimiento**: Implementar particionamiento inteligente, Z-Ordering, o migrar a streaming de datos para an√°lisis casi instant√°neo.
*   **Gobernanza y Monitoreo**: Establecer controles automatizados de calidad de datos y un cat√°logo de datos.
*   **Expansi√≥n a la Plataforma Azure**: Utilizar Azure Data Factory para orquestaci√≥n, Azure Databricks como motor de procesamiento, Azure SQL Database para almacenamiento final, Power BI para visualizaci√≥n, Azure Key Vault para gesti√≥n segura de credenciales, y Azure DevOps para CI/CD.

#### **C√≥mo Ejecutar el Proyecto (Configuraci√≥n)**
Esta secci√≥n es crucial para un portafolio de GitHub, ya que permite a otros replicar tu trabajo.

1.  **Requisitos**:
    *   Una cuenta de **Databricks Free Edition** (o superior).
    *   Acceso a los archivos CSV de 'Uber Pickups in New York City' de Kaggle (disponibles en [https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city](https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city)).
    *   (Opcional) Power BI Desktop para la visualizaci√≥n final.

2.  **Configuraci√≥n del Entorno en Databricks**:
    *   **Crear un cl√∫ster**: Configura un cl√∫ster de Spark compatible con Databricks Runtime.
    *   **Cargar los datos crudos**: Sube los seis archivos CSV (`uber-raw-data-apr14.csv` a `uber-raw-data-sep14.csv`) a una ubicaci√≥n accesible dentro de Databricks (ej. DBFS o un montaje de almacenamiento en la nube). **Aseg√∫rate de ajustar las rutas de origen en el notebook `01_Uber_Bronze_Layer.ipynb` para que apunten a la ubicaci√≥n correcta de tus archivos CSV.**
    *   **Configurar Vol√∫menes**: Si est√°s utilizando la estructura de vol√∫menes de Databricks (como `/Volumes/workspace/default/uber_etl_azure/`), aseg√∫rate de que estos est√©n configurados o adapta las rutas en los notebooks.

3.  **Ejecuci√≥n de los Notebooks**:
    *   **Paso 1: Capa Bronze**
        *   Abre `notebooks/01_Uber_Bronze_Layer.ipynb`.
        *   Ejecuta todas las celdas para ingestar los datos crudos y crear la tabla `uber_bronze`.
    *   **Paso 2: Capa Silver**
        *   Abre `notebooks/02_Uber_Silver_Layer.ipynb`.
        *   Ejecuta todas las celdas para limpiar, validar y enriquecer los datos, creando la tabla `uber_silver`.
    *   **Paso 3: Capa Gold**
        *   Abre `notebooks/03_Uber_Gold_Layer.ipynb`.
        *   Ejecuta todas las celdas para crear las cinco tablas agregadas de la capa Gold (`daily_metrics`, `base_hourly_metrics`, `monthly_kpis`, `spatial_metrics`, `executive_dashboard`).
    *   **Paso 4: Visualizaci√≥n (Databricks)**
        *   Abre `notebooks/04_Uber_Dashboard_Databricks.ipynb`.
        *   Ejecuta las celdas para explorar los datos de la capa Gold y ver visualizaciones preliminares.

4.  **Conexi√≥n con Power BI (Opcional)**:
    *   Conecta Power BI a las tablas Delta de la capa Gold a trav√©s del `Databricks SQL Analytics Endpoint`.


---

